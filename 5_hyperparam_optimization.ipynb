{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "from my_defs import* \n",
    "import os\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "seed = 42\n",
    "jobs = round(cpu_count() / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_down = pd.read_csv('data_down_full.csv')   \n",
    "data_up = pd.read_csv('data_up_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols_u  = [ 'connected', 'date', 'year', 'rawTimesamp',\n",
    "             'month', 'id', 'throughput_var', 'ci',\n",
    "           'tavg', 'tmax', 'tmin', 'wdir' ,'pres', 'tsun','dir',\n",
    "                'campus', 'highway', 'urban', 'suburban',\n",
    "           'Barop', 'Brünninghausen', 'Eichlinghofen', 'Groß-Barop', 'Hombruch', 'Innenstadt Nord',\n",
    "           'KGV Ruhrwaldstraße', 'Kirchhörde', 'Klinikviertel', 'Kruckel', 'Löttringhausen',\n",
    "           'Lücklemberg', 'Mitte' , 'Persebeck', 'Renninghausen', 'Salingen', 'Syburg', 'Wellinghofen',\n",
    "            'Wichlinghofen']\n",
    "\n",
    "drop_cols_d  = ['connected', 'date', 'year','rawTimesamp',\n",
    "             'month', 'id','throughput_var', 'ci',\n",
    "           'tavg', 'tmax', 'tmin', 'wdir' ,'pres', 'tsun','campus', 'highway', 'urban', 'suburban', 'dir',\n",
    "           'Barop', 'Brünninghausen', 'Eichlinghofen', 'Groß-Barop', 'Hombruch', \n",
    "           'KGV Ruhrwaldstraße', 'Kirchhörde', 'Klinikviertel', 'Kruckel', 'Löttringhausen',\n",
    "           'Lücklemberg', 'Mitte' , 'Persebeck', 'Renninghausen', 'Salingen', 'Syburg', 'Wellinghofen',\n",
    "            'Wichlinghofen']\n",
    "            \n",
    "data_down = data_down.drop(drop_cols_d, axis = 1)\n",
    "data_up = data_up.drop(drop_cols_u, axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# possible feature tranformations\n",
    "#data_up['rsrp'] = 10**((data_up['rsrp']/10)/1000)\n",
    "#data_down['rsrp'] = 10**((data_down['rsrp']/10)/1000)\n",
    "\n",
    "#data_up['rsrq'] = 10**(data_up['rsrq']/20)\n",
    "#data_down['rsrq'] = 10**(data_down['rsrq']/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_d,x_test_d,y_train_d,y_test_d=train_test_split(data_down.drop(['throughput'], axis = 1)\n",
    "                                               ,data_down[\"throughput\"], test_size=0.2,\n",
    "                                                       random_state = 42,\n",
    "                                                      shuffle = False)\n",
    "    \n",
    "\n",
    "x_train_u,x_test_u,y_train_u,y_test_u=train_test_split(data_up.drop(['throughput'], axis = 1)\n",
    "                                               ,data_up[\"throughput\"], test_size=0.2, \n",
    "                                                       random_state = 42,\n",
    "                                                      shuffle  =False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Pipeline(steps=[('scaler', None),\n",
    "                        ('regressor', xgb.XGBRegressor())])     \n",
    "\n",
    "pg = [{\n",
    "    'scaler': [\n",
    "       # None,\n",
    "        RobustScaler(),\n",
    "       # StandardScaler(),\n",
    "       # MinMaxScaler()\n",
    "    ],    \n",
    "    'regressor__booster': [ \n",
    "        'gbtree'\n",
    "    ],                       \n",
    "    'regressor__n_estimators': [\n",
    "        105,\n",
    "       # 100,\n",
    "        #110,\n",
    "      #  95,\n",
    "       # 166,\n",
    "       # 65\n",
    "    ],                          \n",
    "    'regressor__max_depth ': [\n",
    "        38,\n",
    "       # None,\n",
    "       # 37, \n",
    "       # 39, \n",
    "        #40,\n",
    "       # 47\n",
    "        \n",
    "    ],\n",
    "    'regressor__learning_rate': [\n",
    "       # 0.1,\n",
    "        #0.12,\n",
    "        0.11,\n",
    "        #0.099\n",
    "\n",
    "    ],\n",
    "    'regressor__min_child_weight': [\n",
    "        #2,\n",
    "       # 3,\n",
    "        #4,\n",
    "     #   9,\n",
    "       # 11,\n",
    "       # 15,\n",
    "        18,\n",
    "        #22,\n",
    "        #25\n",
    "        \n",
    "    ],\n",
    "    'regressor__reg_alpha': [\n",
    "       # 0.55,\n",
    "       # 0.65,\n",
    "        0.6,\n",
    "       # 0.7\n",
    "    ],\n",
    "    'regressor__reg_lambda': [\n",
    "        0.9,\n",
    "       # 0.8,\n",
    "        #0.85,\n",
    "       # 0.95\n",
    "    ],\n",
    "    'regressor__objective': [\n",
    "        'reg:squarederror',\n",
    "       # 'reg:squaredlogerror'\n",
    "    ],   \n",
    "    'regressor__base_score':[\n",
    "        0,\n",
    "        0.5\n",
    "    ], \n",
    "    'regressor__verbosity':[  # kill warnings \n",
    "        0  \n",
    "    ]\n",
    "}] \n",
    "xgb_d = RandomizedSearchCV(\n",
    "    model, scoring='neg_mean_squared_error', \n",
    "    param_distributions=pg, cv=4, refit=True,\n",
    "    n_jobs=jobs - 1, verbose = 0, random_state = seed)    \n",
    "\n",
    "# training\n",
    "xgb_d.fit(x_train_d, y_train_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score:\", (np.sqrt(-xgb_d.best_score_)))\n",
    "display(\"Parameters:\", xgb_d.best_params_)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_xtr_d = scale(x_train_d, x_train_d, RobustScaler())    \n",
    "scaled_xtest_d = scale(x_train_d, x_test_d, RobustScaler())   \n",
    "\n",
    "\n",
    "xgb_down = fit_xgb( scaled_xtr_d, y_train_d, # function in my_defs\n",
    "         objective ='reg:squarederror', booster = \"gbtree\",\n",
    "         n_estimators = 105, max_depth  = 38, learning_rate = 0.11,\n",
    "                           min_child_weight =18 , reg_alpha = 0.6, \n",
    "                          reg_lambda = 0.9, base_score = 0.5, random_state = seed)\n",
    "\n",
    "xgb_score_down = rmse(xgb_down,scaled_xtest_d, y_test_d) # function in my_defs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%%time\n",
    "# pipeline models: Scaler -> RFR\n",
    "model = Pipeline(steps=[('scaler', None),\n",
    "                        ('regressor', xgb.XGBRegressor())])     \n",
    "# grid search params\n",
    "pg = [{\n",
    "    'scaler': [\n",
    "       #None,\n",
    "        RobustScaler()\n",
    "       #StandardScaler(),\n",
    "       #MinMaxScaler()\n",
    "    ],    \n",
    "    'regressor__booster': [ \n",
    "        'gbtree'\n",
    "    ],                       \n",
    "    'regressor__n_estimators': [\n",
    "     #  100,\n",
    "     # 135,\n",
    "      # 125,\n",
    "     #  120,\n",
    "        130,\n",
    "       #150\n",
    "        \n",
    "    ],                          \n",
    "    'regressor__max_depth ': [\n",
    "        #one,\n",
    "       #3,\n",
    "        #,\n",
    "      # 12,\n",
    "      # 17,\n",
    "      #20,\n",
    "        26,\n",
    "      # 31,\n",
    "       #37,\n",
    "       #40,\n",
    "       #60\n",
    "\n",
    "    ],\n",
    "    'regressor__learning_rate': [\n",
    "     #  0.12,\n",
    "       0.11\n",
    "       #0.10,\n",
    "       #0.15\n",
    "    ],\n",
    "   'regressor__min_child_weight': [\n",
    "       #,\n",
    "   #  7,\n",
    "       6\n",
    "     # 10,\n",
    "    #  8\n",
    "   ],\n",
    "   'regressor__reg_alpha': [\n",
    "       0.35\n",
    "     # 0.3,\n",
    "      #0.55,\n",
    "      #.25\n",
    "   ],\n",
    "   'regressor__reg_lambda': [\n",
    "      #0.9,\n",
    "       0.85,\n",
    "      #0.95,\n",
    "      #0.8\n",
    "   ],\n",
    "    'regressor__objective': [\n",
    "        'reg:squarederror'\n",
    "    ],\n",
    "    'regressor__base_score':[0],\n",
    "    'regressor__verbosity':[0]\n",
    "        \n",
    "        }] \n",
    "xgb_u = RandomizedSearchCV(\n",
    "    model, scoring='neg_mean_squared_error', \n",
    "    param_distributions=pg, cv=4,refit=True,\n",
    "    n_jobs=jobs - 1, verbose = 0, random_state = seed)    \n",
    "\n",
    "# training\n",
    "xgb_u.fit(x_train_u, y_train_u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Score:\", np.sqrt(-xgb_u.best_score_))\n",
    "display(\"Parameters:\", xgb_u.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_xtr_u = scale(x_train_u, x_train_u, RobustScaler())\n",
    "scaled_xtest_u = scale(x_train_u, x_test_u, RobustScaler())\n",
    "\n",
    "\n",
    "xgb_up = fit_xgb( scaled_xtr_u, y_train_u, \n",
    "         objective ='reg:squarederror', booster = \"gbtree\",\n",
    "         n_estimators =130, max_depth  = 27, learning_rate = 0.11,\n",
    "                           min_child_weight = 6, reg_alpha = 0.35, \n",
    "                          reg_lambda = 0.85, base_score = 0, random_state = seed)\n",
    "\n",
    "xgb_score_up = rmse(xgb_up,scaled_xtest_u, y_test_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Pipeline(steps=[('scaler', None),\n",
    "                        ('regressor', RandomForestRegressor())])     \n",
    "\n",
    "pg = [{\n",
    "    'scaler': [\n",
    "        None ,\n",
    "      #  StandardScaler(), \n",
    "      #  RobustScaler()\n",
    "    ],    \n",
    "    'regressor__n_estimators': [ \n",
    "       # 128,\n",
    "        125,\n",
    "       # 140,\n",
    "      #  130\n",
    "        \n",
    "        \n",
    "        #115\n",
    "    ],                             \n",
    "    'regressor__max_depth': [\n",
    "        #20,\n",
    "      #  25,\n",
    "       # 15,\n",
    "      #  20,\n",
    "        30,\n",
    "       # 35,\n",
    "       # 40\n",
    "    ],\n",
    "    'regressor__min_samples_split': [\n",
    "        #None,\n",
    "       # 3,\n",
    "        9,\n",
    "        #8,\n",
    "        #10,\n",
    "       # 11\n",
    "        #6\n",
    "       # 8\n",
    "        \n",
    "    ],\n",
    "    'regressor__max_features': [\n",
    "        'auto'\n",
    "    ],\n",
    "    'regressor__ccp_alpha': [\n",
    "        0.0035455\n",
    "        #0.003575, \n",
    "        #0.0035465\n",
    "    ],   \n",
    "    'regressor__max_samples':[\n",
    "      #  None,\n",
    "      #  0.35,\n",
    "      #  0.5,\n",
    "        #0.4,\n",
    "        0.45\n",
    "        #0.25\n",
    "    ]\n",
    "}] \n",
    "rf_d = RandomizedSearchCV(\n",
    "    model, scoring='neg_mean_squared_error', \n",
    "    param_distributions=pg, cv=4, refit=True,\n",
    "    n_jobs=jobs - 1, verbose = 0, random_state = seed)    \n",
    "\n",
    "\n",
    "rf_d.fit(x_train_d, y_train_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score:\", (np.sqrt(-rf_d.best_score_)))\n",
    "display(\"Parameters:\", rf_d.best_params_)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled_xtr = scale(x_train_d, x_train_d, StandardScaler())\n",
    "#scaled_xtest = scale(x_train_d, x_test_d, StandardScaler())\n",
    "\n",
    "\n",
    "rf_down = fit_rf( x_train_d, y_train_d, n_estimators = 125, max_depth = 30,\n",
    "             min_samples_split = 9,max_samples = 0.45,\n",
    "                 ccp_alpha = 0.0035455, random_state = seed)\n",
    "\n",
    "rf_score_down = rmse(rf_down,x_test_d, y_test_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Pipeline(steps=[('scaler', None),\n",
    "                        ('regressor', RandomForestRegressor())])     \n",
    "\n",
    "pg = [{\n",
    "    'scaler': [\n",
    "        None \n",
    "    #   StandardScaler(), \n",
    "     #  RobustScaler()\n",
    "    ],    \n",
    "    'regressor__n_estimators': [ \n",
    "        #20,\n",
    "        150\n",
    "        #00,\n",
    "        #25,\n",
    "        #15,\n",
    "        \n",
    "       #110,\n",
    "       #90,\n",
    "      # 130,\n",
    "        #40\n",
    "\n",
    "    ],                             \n",
    "    'regressor__max_depth': [\n",
    "     #  None,\n",
    "     #  16,\n",
    "        65\n",
    "        #9,\n",
    "        #7\n",
    "     ## 20,\n",
    "     #  25\n",
    "      # \n",
    "        \n",
    "    ],\n",
    "    'regressor__min_samples_split': [\n",
    "        \n",
    "       #10,\n",
    "       #3,\n",
    "       5,\n",
    "      # 4\n",
    "        #,\n",
    "        #0\n",
    "        #6\n",
    "    ],\n",
    "    'regressor__max_features': [\n",
    "        'auto'\n",
    "    ],\n",
    "    'regressor__max_samples':[\n",
    "        #one,\n",
    "       #0.5,\n",
    "        0.7,\n",
    "       #0.65,\n",
    "       #0.75\n",
    "    ],\n",
    "    'regressor__ccp_alpha':[\n",
    "\n",
    "        0.001462\n",
    "\n",
    "    ]\n",
    "     \n",
    "}] \n",
    "rf_u = RandomizedSearchCV(\n",
    "    model, scoring='neg_mean_squared_error', \n",
    "    param_distributions=pg, cv=4,refit=True,\n",
    "    n_jobs=jobs - 1, verbose = 0, random_state = seed)    \n",
    "\n",
    "# training\n",
    "rf_u.fit(x_train_u, y_train_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score:\", (np.sqrt(-rf_u.best_score_)))\n",
    "display(\"Parameters:\", rf_u.best_params_)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled_xtr = scale(x_train_u, x_train_u, RobustScaler())\n",
    "#scaled_xtest = scale(x_train_u, x_test_u, RobustScaler())\n",
    "\n",
    "\n",
    "rf_up = fit_rf( x_train_u, y_train_u, n_estimators = 150, max_depth = 65,\n",
    "             min_samples_split = 4, max_samples = 0.7,\n",
    "               ccp_alpha =0.001462,random_state = seed )\n",
    "\n",
    "rf_score_up = rmse(rf_up,x_test_u, y_test_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xgb_down.predict(scaled_xtest_d)\n",
    "visualize_prediction_value_ordered_examples(y_test_d, pred,  # function in my_defs\n",
    "                                            title = 'XGBoost Download Predictions and Ground Truth Values',\n",
    "                                           score = xgb_score_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xgb_up.predict(scaled_xtest_u)\n",
    "visualize_prediction_value_ordered_examples(y_test_u, pred, # function in my_defs\n",
    "                                            title = 'XGBoost Upload Predictions and Ground Truth Values',\n",
    "                                            score = xgb_score_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf_down.predict(x_test_d)\n",
    "visualize_prediction_value_ordered_examples(y_test_d, pred, \n",
    "                                            title = 'Random Forrest Download Predictions and Ground Truth Values',\n",
    "                                           score = rf_score_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf_up.predict(x_test_u)\n",
    "visualize_prediction_value_ordered_examples(y_test_u, pred,\n",
    "                                            title = 'Random Forrest Upload Predictions and Ground Truth values',\n",
    "                                           score = rf_score_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    display(Image(filename='rf_learning_curve.png'))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    _, axes = plt.subplots(1,2, figsize=(35, 18))\n",
    "    plot_learning_curves( # function in my_defs \n",
    "       rf_u, x_train_u, y_train_u,random_state = seed,\n",
    "        train_sizes=np.linspace(0.01, 1.0, 50), cv=3,\n",
    "        scoring='neg_root_mean_squared_error', model_name='RandomForrest Learning Curve (Upload)', \n",
    "        ax=axes[0], n_jobs=-1)\n",
    "    plot_learning_curves( # function in my_defs\n",
    "        rf_d, x_train_d, y_train_d,random_state = seed,\n",
    "        train_sizes=np.linspace(0.01, 1.0, 50), cv=3,\n",
    "        scoring='neg_root_mean_squared_error', model_name='RandomForrest Learning Curve (Download)', \n",
    "        ax=axes[1], n_jobs=-1)\n",
    "    plt.savefig('rf_learning_curve' + '.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    display(Image(filename='xgb_learning_curve.png'))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    _, axes = plt.subplots(1,2, figsize=(35, 18))\n",
    "    plot_learning_curves( # function in my_defs\n",
    "        xgb_u, x_train_u, y_train_u,random_state = seed,\n",
    "        train_sizes=np.linspace(0.01, 1.0, 40), cv=3,\n",
    "        scoring='neg_root_mean_squared_error', model_name='XGBoost Learnong Curve (Upload)', \n",
    "        ax=axes[0], n_jobs=-1)\n",
    "    plot_learning_curves( # function in my_defs\n",
    "        xgb_d, x_train_d, y_train_d,random_state = seed,\n",
    "        train_sizes=np.linspace(0.01, 1.0, 40), cv=3,\n",
    "        scoring='neg_root_mean_squared_error', model_name='XGBoost Learning Curve (Download)', \n",
    "        ax=axes[1], n_jobs=-1)\n",
    "    plt.savefig('xgb_learning_curve' + '.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
