{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from my_defs import* \n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_down = pd.read_csv('Data_down.csv', sep='\\s+')   #your oath here \n",
    "data_up = pd.read_csv('Data_up.csv', sep='\\s+')    #your oath here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datetime-related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting rawTimesamp from Unix to Datetime\n",
    "data_up['date'] = data_up['rawTimesamp'] \n",
    "data_down['date'] = data_down['rawTimesamp']\n",
    "\n",
    "data_down['rawTimesamp'] = data_down['rawTimesamp'].map(   #adding '%Y-%m-%d %H:%M:%S'\n",
    "    lambda x: unix_to_datetime(x))   # function in my_defs\n",
    "\n",
    "data_up['rawTimesamp'] = data_up['rawTimesamp'].map(\n",
    "    lambda x: unix_to_datetime(x)) # function in my_defs\n",
    "\n",
    "\n",
    "data_down['date'] = data_down['date'].map(    #adding '%Y-%m-%d'\n",
    "    lambda x: unix_to_date(x))   # function in my_defs\n",
    "\n",
    "data_up['date'] = data_up['date'].map(\n",
    "    lambda x: unix_to_date(x))   # function in my_defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking data types after date-transformations\n",
    "print('Download Data Types\\n',data_down.dtypes)\n",
    "print('Upload Data Types\\n',data_up.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the cols: hour, week, year, dayofweek, dayofmonth and month\n",
    "# and categorically encoding daytime\n",
    "\n",
    "\n",
    "# the definition 'encode_day_time_cat' calls 'featurize_datetime' inside it  \n",
    "\n",
    "data_up = encode_day_time_cat(data_up, 'hour', 'day_time')   # function in my_defs\n",
    "data_down = encode_day_time_cat(data_down, 'hour', 'day_time')   # function in my_defs\n",
    "\n",
    "data_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the col rush_hour\n",
    "'''\n",
    "this first function returns 1 if time is between [06-09 or 16-19]\n",
    "or 0 otherwise\n",
    "'''\n",
    "data_up['rush_hour'] = data_up['hour'].map(\n",
    "    lambda x: 1 if x >= 6 and x< 9 else (1 if x >= 16 and x< 9 else 0 ))  \n",
    "data_down['rush_hour'] = data_down['hour'].map(\n",
    "    lambda x: 1 if x >= 6 and x< 9 else (1 if x >= 16 and x< 9 else 0 )) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unique_values(dataset, u_col = \"\"):\n",
    "    print('unique values', u_col , '=', len(data_down[u_col].unique()), '\\n',\n",
    "         data_down[u_col].unique(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_unique_values(data_down,u_col = \"hour\" )\n",
    "print_unique_values(data_down,u_col = \"week\" )\n",
    "print_unique_values(data_down,u_col = \"dayofweek\" )\n",
    "print_unique_values(data_down,u_col = \"month\" )\n",
    "print_unique_values(data_down,u_col = \"dayofmonth\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_unique_values(data_up,u_col = \"hour\" )\n",
    "print_unique_values(data_up,u_col = \"week\" )\n",
    "print_unique_values(data_up,u_col = \"dayofweek\" )\n",
    "print_unique_values(data_up,u_col = \"month\" )\n",
    "print_unique_values(data_up,u_col = \"dayofmonth\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Upload Measurements count in 2018:',\n",
    "      data_up[data_up['year'] == 2018].groupby('measurement').count().shape[0])\n",
    "\n",
    "print('Upload Measurements count in 2019:',\n",
    "      data_up[data_up['year'] == 2019].groupby('measurement').count().shape[0])\n",
    "\n",
    "print('Download Measurements count in 2018:',\n",
    "      data_down[data_down['year'] == 2018].groupby('measurement').count().shape[0])\n",
    "\n",
    "print('Download Measurements count in 2019:',\n",
    "      data_down[data_down['year'] == 2019].groupby('measurement').count().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Weather Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = pd.read_csv('DE-8GBU_18_19.csv', parse_dates=['date'])\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dates = data_up['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.loc[weather_data['date'].isin(unique_dates)].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = weather_data.drop(['wpgt', 'snow'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_up = pd.merge(data_up,weather_data, how='inner', on='date' )\n",
    "data_down = pd.merge(data_down,weather_data, how='inner', on='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_up = onehot_enc(data_up, 'location') # function in my_defs\n",
    "data_down = onehot_enc(data_down, 'location')# function in my_defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Rolling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_down = data_down[['rawTimesamp', 'measurement',\n",
    "                          'throughput']].sort_values(by=['measurement','rawTimesamp'] )\n",
    "rolling_up = data_up[['rawTimesamp', 'measurement', \n",
    "                      'throughput']].sort_values(by=['measurement','rawTimesamp'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_down = means_and_std(rolling_down,rolling_col = 'measurement',\n",
    "                            target_col = 'throughput')  # function in my_defs\n",
    "rolling_up = means_and_std(rolling_up, rolling_col = 'measurement',\n",
    "                           target_col = 'throughput')# function in my_defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function in my_defs\n",
    "rolling_down = shift_group_features_(rolling_down, group_by ='measurement',\n",
    "                                  shift_col1 = \"throughput_mean\", shift_col2 = \"throughput_std\",\n",
    "                                    shift_col3 = \"throughput_var\")\n",
    "# function in my_defs\n",
    "rolling_up = shift_group_features_(rolling_up, group_by ='measurement',\n",
    "                               shift_col1 = \"throughput_mean\", shift_col2 = \"throughput_std\",\n",
    "                                    shift_col3 = \"throughput_var\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_down = pd.merge(data_down,rolling_down, how='inner')\n",
    "data_up = pd.merge(data_up,rolling_up, how='inner')\n",
    "data_down.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add address  as OneHot Feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "**This block of code may take quite some time till completion, depending on your internet speed \n",
    "(check my wall time in cell [25] before running it)\n",
    "\n",
    "Don't want to wait ? :D \n",
    "\n",
    "Just navigate to the last cell of this section [29] and uncomment it to read and view our csv-files \n",
    "containing the one-hot-encoded addresses.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_up['coordinates'] = list(zip(data_up.lat, data_up.lon))\n",
    "data_down['coordinates'] = list(zip(data_down.lat, data_down.lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_up['dist'] = data_up['coordinates'].map(\n",
    "    lambda x:get_district(x)) # function in my_defs\n",
    "\n",
    "data_down['dist'] = data_down['coordinates'].map(\n",
    "    lambda x:get_district(x)) # function in my_defs \n",
    "\n",
    "data_up = data_up.drop([\"coordinates\"], axis =1 )\n",
    "data_down = data_down.drop([\"coordinates\"], axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_down = onehot_enc(data_down, \"dist\") # function in my_defs\n",
    "data_up = onehot_enc(data_up, \"dist\") # function in my_defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the data files as data_file_fat.csv in order to save time for next runs!\n",
    "\n",
    "data_down.to_csv(\"data_down_fat.csv\", index= False)  #your oath here\n",
    "data_up.to_csv(\"data_up_fat.csv\", index = False)  #your oath here  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "UNCOMMENT if you don't wanna wait to download the districts\n",
    "and check your path if you get an error\n",
    "'''\n",
    "\n",
    "#data_down = pd.read_csv(\"data_down_fat.csv\", parse_dates = [\"rawTimesamp\", \"date\"])\n",
    "#data_up= pd.read_csv(\"data_up_fat.csv\", parse_dates = [\"rawTimesamp\", \"date\"])\n",
    "#display(data_down.head())\n",
    "#display(data_up.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pysical Cell ID Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_down = onehot_enc(data_down, 'pci')\n",
    "data_up = onehot_enc(data_up, 'pci') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_up_full.csv & data_down_full.csv  have all the generated features.\n",
    "# all undesired features will be right before modeling dropped\n",
    "\n",
    "\n",
    "data_up.to_csv(\"data_up_full.csv\", index = False)  #your path here\n",
    "\n",
    "data_down.to_csv(\"data_down_full.csv\", index= False)  #your path here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
